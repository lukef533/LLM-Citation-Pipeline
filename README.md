# Automated Extraction & Classification of Dataset Citations  

I developed this project for the [Make Data Count](https://makedatacount.org/) Kaggle competition, which focuses on identifying and classifying dataset citations from scientific literature. The challenge centered on **PDF-based biomedical research papers**, and my solution was a fully automated **natural language processing (NLP)** pipeline that integrates **regex-based extraction** with **Large Language Model (LLM)** classification.  

## ðŸ“„ Project Structure & Pipeline Design  

The pipeline follows these main stages:  

1. **PDF Parsing**  
   - Extract text from research PDFs using `PyMuPDF` (`fitz`).  
   - Segment text into overlapping context windows with `nltk` sentence tokenization to ensure no citation is missed.  

2. **Candidate Identification**  
   - Use **regular expressions** to capture DOI-like strings, maximizing recall.  

3. **Citation Classification with LLMs**  
   - Load Hugging Face LLMs (`Qwen2.5` series) with the `transformers` library.  
   - Apply **prompt engineering** and **few-shot prompting** to classify candidates into:  
     - **Primary** â€“ Data generated by the paper  
     - **Secondary** â€“ Reused data from previous work  
     - **Irrelevant** â€“ Non-dataset DOIs  

4. **Post-Processing**  
   - Remove self-citations, preprints, and duplicates.  
   - Output a clean `(article_id, dataset_id)` mapping for submission.  

---

## ðŸŽ¯ Final Product  

The result is a **modular, competition-ready NLP pipeline** that:  
- Parses and cleans PDF-based scientific articles  
- Uses a hybrid **regex + LLM** approach for dataset citation extraction  
- Classifies citations with contextual LLM reasoning  
- Produces deduplicated, labeled outputs for evaluation  

Through iterative improvements in regex patterns, prompt design, and filtering logic, I increased my F1 score from **0.05 to 0.229**â€”a significant performance boost from the baseline.  

---

## ðŸ›  Key Python Packages Used  

- **Core utilities:** `os`, `pathlib`, `re`, `collections.Counter`, `random`  
- **Data handling:** `pandas`, `numpy`  
- **PDF parsing:** `fitz` (PyMuPDF)  
- **Text processing:** `nltk` (`sent_tokenize`)  
- **Plotting & analysis:** `matplotlib`  
- **LLM & NLP:** `transformers`, `torch`, Hugging Face pipelines  
- **Progress tracking:** `tqdm`  

---

## ðŸ“Œ Takeaways  

By combining **deterministic pattern matching** for high recall with **LLM-based semantic classification** for high precision, I created a reproducible and scalable approach to dataset citation extraction. This framework can easily extend to other **Named Entity Recognition (NER)** and classification tasks across unstructured text domains.  
